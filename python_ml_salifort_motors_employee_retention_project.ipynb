{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n\n![Screenshot 2022-08-03 2.16.00 PM.png](data:image/png;base64,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)\n\n# Pace: Plan Stage\n\n- Understand your data in the problem context\n- Consider how your data will best address the business need\n- Contextualize & understand the data and the problem\n","metadata":{"id":"psz51YkZVwtN"}},{"cell_type":"markdown","source":"ðŸ—’\n### Understand the business scenario and problem\n\nThe HR department at Salifort Motors wants to take some initiatives to improve employee satisfaction levels at the company. They collected data from employees, but now they donâ€™t know what to do with it. They refer to you as a data analytics professional and ask you to provide data-driven suggestions based on your understanding of the data. They have the following question: whatâ€™s likely to make the employee leave the company?\n\nYour goals in this project are to analyze the data collected by the HR department and to build a model that predicts whether or not an employee will leave the company.\n\nIf you can predict employees likely to quit, it might be possible to identify factors that contribute to their leaving. Because it is time-consuming and expensive to find, interview, and hire new employees, increasing employee retention will be beneficial to the company.","metadata":{"id":"gLEEr6peWcF7"}},{"cell_type":"markdown","source":"### Familiarize yourself with the HR dataset\n\nThe dataset that you'll be using in this lab contains 15,000 rows and 10 columns for the variables listed below. \n\n**Note:** For more information about the data, refer to its source on [Kaggle](https://www.kaggle.com/datasets/mfaisalqureshi/hr-analytics-and-job-prediction?select=HR_comma_sep.csv).\n\nVariable  |Description |\n-----|-----|\nsatisfaction_level|Employee-reported job satisfaction level [0&ndash;1]|\nlast_evaluation|Score of employee's last performance review [0&ndash;1]|\nnumber_project|Number of projects employee contributes to|\naverage_monthly_hours|Average number of hours employee worked per month|\ntime_spend_company|How long the employee has been with the company (years)\nWork_accident|Whether or not the employee experienced an accident while at work\nleft|Whether or not the employee left the company\npromotion_last_5years|Whether or not the employee was promoted in the last 5 years\nDepartment|The employee's department\nsalary|The employee's salary (U.S. dollars)","metadata":{"id":"lnRdR6eacUkK"}},{"cell_type":"markdown","source":"## Step 1. Imports\n\n*   Import packages\n*   Load dataset\n\n","metadata":{"id":"Dfnvw1HZsfbd"}},{"cell_type":"markdown","source":"### Import packages","metadata":{"id":"51UAXIOLC_8P"}},{"cell_type":"code","source":"# Import packages\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', None)\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,\\\nconfusion_matrix, ConfusionMatrixDisplay, classification_report\nfrom sklearn.metrics import roc_auc_score, roc_curve","metadata":{"id":"hVWGpX9As4e1","execution":{"iopub.status.busy":"2023-06-05T20:39:15.126357Z","iopub.execute_input":"2023-06-05T20:39:15.126737Z","iopub.status.idle":"2023-06-05T20:39:16.396064Z","shell.execute_reply.started":"2023-06-05T20:39:15.126708Z","shell.execute_reply":"2023-06-05T20:39:16.394800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Load dataset","metadata":{"id":"zM2P9yLWDIjN"}},{"cell_type":"code","source":"# Load dataset into a dataframe\ndf0 = pd.read_csv(\"/kaggle/input/hr-analytics-and-job-prediction/HR_comma_sep.csv\")\n\n\n# Display first few rows of the dataframe\ndf0.head()","metadata":{"id":"Bs0cJR5BDPgQ","execution":{"iopub.status.busy":"2023-06-05T20:40:04.363464Z","iopub.execute_input":"2023-06-05T20:40:04.363942Z","iopub.status.idle":"2023-06-05T20:40:04.403961Z","shell.execute_reply.started":"2023-06-05T20:40:04.363910Z","shell.execute_reply":"2023-06-05T20:40:04.402698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. Data Exploration (Initial EDA and data cleaning)\n\n- Understand your variables\n- Clean the dataset (missing data, redundant data, outliers)\n\n","metadata":{"id":"wF_LLorPs5G_"}},{"cell_type":"markdown","source":"### Gather basic information about the data","metadata":{"id":"3LF6h1v9FYz2"}},{"cell_type":"code","source":"# Gather basic information about the data\ndf0.info()","metadata":{"id":"6XbfdPoKurMf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Gather descriptive statistics about the data","metadata":{"id":"6JMl_rQ1Fgte"}},{"cell_type":"code","source":"# Gather descriptive statistics about the data\ndf0.describe()","metadata":{"id":"_5VRL-kzE8y1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Rename columns","metadata":{"id":"QR7eFNU0FklJ"}},{"cell_type":"markdown","source":"Standardize the column names so that they are all in `snake_case`, correct any column names that are misspelled, and make column names more concise as needed.","metadata":{"id":"_TtE0JLPJyLF"}},{"cell_type":"code","source":"# Display all column names\ndf0.columns","metadata":{"id":"kEn21u2bqrEI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename columns as needed\ndf0 = df0.rename(columns = {'Work_accident': 'work_accident',\n                           'average_montly_hours': 'average_monthly_hours',\n                           'Department': 'department',\n                           'time_spend_company': 'tenure'})\n\n# Display all column names after the update\ndf0.columns\n","metadata":{"id":"npUQA8jMFJQD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check missing values","metadata":{"id":"e-G2QlQxBq__"}},{"cell_type":"markdown","source":"Check for any missing values in the data.","metadata":{"id":"GeiUUqeaBt-I"}},{"cell_type":"code","source":"# Check for missing values\ndf0.isnull().sum()","metadata":{"id":"EN9MvN0GByVV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check duplicates","metadata":{"id":"hBvrijItKQI9"}},{"cell_type":"markdown","source":"Check for any duplicate entries in the data.","metadata":{"id":"Q7ystBsdsGaL"}},{"cell_type":"code","source":"# Check for duplicates\ndf0.duplicated().sum()","metadata":{"id":"CFFLc5AOZ7-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect some rows containing duplicates as needed\ndf0[df0.duplicated()]","metadata":{"id":"ZHGlDbKAcBLM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop duplicates and save resulting dataframe in a new variable as needed\ndf1 = df0.drop_duplicates(keep = 'first')\n\n# Display first few rows of new dataframe as needed\ndf1.head()\n","metadata":{"id":"wCr34Rppdjay"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Check outliers","metadata":{"id":"4knHIoTIFu83"}},{"cell_type":"markdown","source":"Check for outliers in the data.","metadata":{"id":"EaVx2fk8GC_m"}},{"cell_type":"code","source":"# Create a boxplot to visualize distribution of `tenure` and detect any outliers\nsns.boxplot(x = 'tenure', data = df1)\nplt.title('Boxplot Outliers tenure')\nplt.show()\n","metadata":{"id":"pilaGYgh4LHM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Determine the number of rows containing outliers\npercentile25 = df1['tenure'].quantile(0.25)\npercentile75 = df1['tenure'].quantile(0.75)\n\niqr = percentile75 - percentile25\n\nupper_limit = percentile75 + 1.5 * iqr\nlower_limit = percentile25 - 1.5 * iqr\nprint('lower limit:', lower_limit)\nprint('upperlimit:', upper_limit)\n\noutliers = df1[(df1['tenure'] < lower_limit) | (df1['tenure'] > upper_limit)]\nprint('')\nprint('Total Outliers:', len(outliers))","metadata":{"id":"ohctgiHyFykI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Certain types of models are more sensitive to outliers than others. When getting to the stage of building your model, we will consider whether to remove outliers, based on the type of model you decide to use.","metadata":{"id":"eP_rPN31Kmx0"}},{"cell_type":"markdown","source":"![Screenshot 2022-08-04 1.34.26 PM.png](data:image/png;base64,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)\n# pAce: Analyze Stage\n- Perform EDA (analyze relationships between variables)\n\n","metadata":{"id":"mA7Mz_SnI8km"}},{"cell_type":"markdown","source":"## Step 2. Data Exploration (Continue EDA)","metadata":{"id":"KDcWrk57kao2"}},{"cell_type":"markdown","source":"### Data visualizations","metadata":{"id":"DmVMzXPSuYk1"}},{"cell_type":"markdown","source":"Examining the variables that we are interested in, and create plots to visualize relationships between variables in the data.","metadata":{"id":"_hw4qkZzBr-W"}},{"cell_type":"markdown","source":"We will start by ploting:\n1. A Boxplot where we can see the distribution between the avg_monthly_hours vs the number of projects employees have, from employees who stayed and left.\n2. A histogram that shows the distribution of number of project employees have, differenciated between the ones who stayed and the ones who left.","metadata":{}},{"cell_type":"code","source":"# Create a plot\nfig, ax = plt.subplots(1, 2, figsize = (22, 8))\n\nsns.boxplot(x = 'average_monthly_hours',\n           y = 'number_project',\n           data = df1,\n            hue = 'left',\n            orient = 'h',\n           ax = ax[0])\nax[0].invert_yaxis()\nax[0].set_title('Monthly Hours vs Number of Projects', fontsize = '22')\n\n\nsns.histplot(data = df1, \n             x = 'number_project',\n             hue = 'left', \n             multiple = 'dodge',\n             shrink = 2,\n             ax = ax[1])\nax[1].set_title('Number of Projects Histogram', fontsize = '22')\nax[1].legend(labels = ['left', 'stayed'])\n\nplt.show()","metadata":{"id":"Qf0VbjX8-DBQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the previous plots we can notice the following:\n1. We can notice two groups. Those who work less than their peers with the same number of projectos (A), and the other one comprised of those who work much more (B).\nWe should consider the in group A they probably were fired and also it can contain people who had already resign, and therefore have less responsabilities. \nIn group B, its reasonable that they have resign.\n2. As the number of projects starts to increase above 3, you start to see more employees leaving. Getting to the point where all the employees who have 7 projects have left.\n3. The ratio of left/stay is much lower when projects are between 3 and 4 per employee.\n4. The avg monthly hours is above from common standarts. Assuming that the month have 21 working days, and that usually a person works 8 hours per day, its a total of 168 hours/month on average.","metadata":{}},{"cell_type":"code","source":"# Create a plot\nplt.figure(figsize = (16, 9))\nsns.scatterplot(x = 'average_monthly_hours',\n               y = 'satisfaction_level',\n               data = df1,\n               hue = 'left',\n               alpha = 0.4)\nplt.title('Satisfaction Level vs Avg. Monthly Hours', fontsize = '18')\nplt.axvline(x = 168, color = 'Red', label = '168 hrs/month', ls = '--')\nplt.legend(labels = ['168 hrs/month','left', 'stayed'])\nplt.show()","metadata":{"id":"F8HlhjMy9X3A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can notice the following form the previous plot:\n1. There is a considerable group of people who left the company that worked ~240-350 hours/month that had very low satisfaction levels.\n2. Another cluster of employees who left worked less than the standard avg. hours/month and their satisfaction level was low.\n3. Finally theres a cluster that worked high avg monthly hours, had high satisfaction levels, and left the company.\n\n\nNotice the strange shape of the distribution of the observation in the plot, its indicative of data manipulation or synthetic data for the following reasons:\n1. Unnatural Clusters: data points form unnaturally perfect clusters, real world data tends to have more irregular and diverse clustering patterns.\n2. Uniform Spacing: data points are uniformaley arrange in a highly regular pattern. In real world scenarios data points are often distributed unevenly.","metadata":{}},{"cell_type":"markdown","source":"Now we will plot satisfaction_levels and tenure:","metadata":{}},{"cell_type":"code","source":"# Create a plot\nfig, ax = plt.subplots(1, 2, figsize = (22, 8))\n\nsns.boxplot(x = 'satisfaction_level',\n           y = 'tenure',\n           data = df1,\n           hue = 'left',\n           orient = 'h',\n           ax = ax[0])\n\nax[0].invert_yaxis()\nax[0].set_title('Boxplot Satisfaction Level vs Tenure', fontsize = '22')\n\nsns.histplot(data = df1,\n            x = 'tenure',\n            hue = 'left',\n            multiple = 'dodge',\n            shrink = 5,\n            ax = ax[1])\n\nax[1].set_title('Tenure Histogram', fontsize = '22')\nax[1].legend(labels = ['left', 'stayed'])\n\nplt.show()\n","metadata":{"id":"NUyBruMee-EI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Our observations:\n1. There are 2 categories of employees who left. The ones with medium length tenure and high satisfaction levels, and the ones with shorter tenure and low satisfaction levels. \n2. Theres a group of employees with 4 year tenure, who left and where really dissatisfied. ItÂ´s worth doing some research of what could be affecting this.\n3. Satisfaction levels of low tenure and high tenure is very similar.\n4. Most employees are between 2 and 4 tenure length. There are very few over 6 years tenure.\n5. Once the employees have more than 3 years tenure the number of employees that leave decreases. Over 6 years tenure the are no employees who left the company.","metadata":{}},{"cell_type":"markdown","source":"We will calculte the mean and median satisfaction level of employees who stay and left the company.","metadata":{}},{"cell_type":"code","source":"# Create a plot\ndf1.groupby(by = 'left')['satisfaction_level'].agg({'mean', 'median'})","metadata":{"id":"3v1uJR5y3MEy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The mean of the employees who left is ~23% lower than the ones who stayed. This distribution of of the ones who stayed seems to be somewhat skewed to the right, and the distribution from the ones who stayed somewhat skewed to the left.","metadata":{}},{"cell_type":"markdown","source":"We shall proceed eximining the salaray for different tenures.","metadata":{}},{"cell_type":"code","source":"# Create a plot\nfig, ax = plt.subplots(1, 2, figsize = (22, 8))\n\ntenure_short = df1[df1['tenure'] < 7]\ntenure_long = df1[df1['tenure'] >= 7]\n\nsns.histplot(data = tenure_short,\n           x = 'tenure',\n           hue = 'salary', \n             hue_order = ['low', 'medium', 'high'],\n           multiple = 'dodge',\n           shrink = 6,\n            ax = ax[0])\n\nax[0].set_title('Salary Histogram by Tenure: short-tenured employees', fontsize = '18')\n\nsns.histplot(data = tenure_long,\n            x = 'tenure',\n            hue = 'salary',\n             hue_order = ['low', 'medium', 'high'],\n            multiple = 'dodge',\n            shrink = 1,\n            ax = ax[1])\n\nax[1].set_title('Salary Histogram by Tenure: long-tenured employees', fontsize = '18')\n\n\nplt.show()","metadata":{"id":"UCVs81NILbhn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We notice the following:\n1. For tenures below 7 years, the distribution of between salaries is pretty similar. \n2. For tenures over 7 years, we can notice that medium higher than high, and high is bigger than low. \n3. For long tenure emplyees, high salary doesnÂ´t grow disproportionaly.","metadata":{}},{"cell_type":"markdown","source":"We will explore if there is any correlation between working long hours and evaluation scores.","metadata":{}},{"cell_type":"code","source":"# Create a plot\nplt.figure(figsize = (16, 9))\nsns.scatterplot(data = df1,\n               x = 'average_monthly_hours',\n               y = 'last_evaluation',\n               hue = 'left',\n               alpha = 0.4)\n\nplt.axvline(x = 168, color = 'Red', ls = '--')\nplt.legend(labels = ['168 hrs/month' ,'left', 'stayed'])\nplt.title('Monthly Hours by Last Evaluation Score', fontsize= '18')\n","metadata":{"id":"cGitCvzvdbjF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n1. There are 2 clusters in the employees who left. The ones the worked long hours and had high last evaluation score, and the ones who worked less hours than the standard avg. monthly hours and had low score in their las evaluation. \n2. There seems to be some positive correlation between hours worked and evaluation scores.\n3. There isnÂ´t a high percentage of points in the higher eft quadrant, so working long hours doesnÂ´t assure a high evaluation score.","metadata":{}},{"cell_type":"markdown","source":"We will explore whether the employees who worked long hours were promoted in the las 5 years.","metadata":{}},{"cell_type":"code","source":"# Create a plot\nplt.figure(figsize = (16, 3))\n\nsns.scatterplot(data = df1,\n               x = 'average_monthly_hours',\n               y = 'promotion_last_5years',\n               hue = 'left',\n               alpha = 0.4)\n\nplt.axvline(x = 168, color = 'Red',ls = '--')\nplt.legend(labels = ['168 hrs/month', 'left', 'stayed'])\nplt.title('Promotion in last 5 years by Avg. Monthly Hours', fontsize = '18')\n\nplt.show()\n","metadata":{"id":"6TyBo1uxsSpc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n1. Very fue employees were promoted in the last few years.\n2. Almost no employee who worked the longest hours were promoted in the last few years.\n3. Almost everyone who left wasnÂ´t promoted in the last 5 years.\n4. Very few eomplyees who left were promoted in the las 5 years.","metadata":{}},{"cell_type":"markdown","source":"Explore the distribution of employees whi left across departments.","metadata":{}},{"cell_type":"code","source":"# Create a plot\nplt.figure(figsize = (11, 8))\n\nsns.histplot(data = df1,\n            x = 'department',\n            hue = 'left',\n            multiple = 'dodge',\n            shrink = .5)\n\nplt.xticks(rotation = 45)\nplt.legend(labels = ['left', 'stayed'])\nplt.title('Distribution left/stayed by Department', fontsize = '18')\n\nplt.show()","metadata":{"id":"lfo96dwwruZd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There doesnÂ´t seem to be significant changes in the distribution of left/stayed employees across departments.","metadata":{}},{"cell_type":"markdown","source":"Lastly, we are going to analize correlation between variables in the dataframe.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (16, 9))\n\nsns.heatmap(data = df0.corr(),\n            vmin = -1,\n            vmax = 1,\n            annot = True,\n           cmap = sns.color_palette('vlag', as_cmap = True)\n           )\n\nplt.title('Correlation between variables Heatmap', fontsize = '18', pad = 20)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n1. The correlation heatmap shows us that there is some positive correlation between average monthly hours, number of projects and evaluation score. \n2. There is a negative correlation between satisfaction level and the people who leave the company.","metadata":{}},{"cell_type":"markdown","source":"### Insights","metadata":{"id":"DeTmNVlAANLd"}},{"cell_type":"markdown","source":"1. There seems to that employees could be leaving because of long hours, many projects and low satisfaction levels. They are probably not comfortable working long hours and not receiving high evaluation scores or promotions during 5 years.\n2. If they have more than 6 year tenure they tend not to leave the company.","metadata":{"id":"sQT8YqymD-yL"}},{"cell_type":"markdown","source":"![Screenshot 2022-08-04 2.03.22 PM.png](data:image/png;base64,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)\n# paCe: Construct Stage\n- Determine which models are most appropriate\n- Construct the model\n- Confirm model assumptions\n- Evaluate model results to determine how well your model fits the data\n","metadata":{"id":"Lca9c8XON8lc"}},{"cell_type":"markdown","source":"## Step 3. Model Building, Step 4. Results and Evaluation\n- Fit a model that predicts the outcome variable using two or more independent variables\n- Check model assumptions\n- Evaluate the model","metadata":{"id":"ZDG9v-NCS69j"}},{"cell_type":"markdown","source":"### Identify the type of prediction task.","metadata":{"id":"QxCXjdpSaB8A"}},{"cell_type":"markdown","source":"Our objective is to predict which employees will leave the company, determining which variables affect the most for an employee to leave the company.\nOur dependent variable is categorical and binary (leave or not leave).","metadata":{"id":"YcCoHhnKcEov"}},{"cell_type":"markdown","source":"### Identify the types of models most appropriate for this task.","metadata":{"id":"yeUlo3W0ais-"}},{"cell_type":"markdown","source":"Since the dependent variable is categorical and binary, we should focus on Logistic Regression model or a Tree Based model (Machine Learning model).\nSo we will evaluate both to determine which one is more accurate to our objective and the context of our scenario.\n\n\nWe shall start with Logistic Regression and then proceed with Tree Based modeling.","metadata":{"id":"A0Mo6f9tOchC"}},{"cell_type":"markdown","source":"### Modeling","metadata":{"id":"OrW2oXy4OfD1"}},{"cell_type":"markdown","source":"#### Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"We will star by hot-encoding de categorical variables.","metadata":{}},{"cell_type":"code","source":"df_enc = pd.get_dummies(df1, prefix = ['salary', 'dept'], columns = ['salary', 'department'] ,drop_first = True)\n\ndf_enc.head()","metadata":{"id":"UePZZyi_Okdz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We shall create a heatmap to analize correlation between variables we are interested.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (8, 6))\n\ninterested_var = ['satisfaction_level', 'last_evaluation', 'number_project', 'average_monthly_hours', 'tenure']\n\nsns.heatmap(df_enc[interested_var].corr(), vmin = -1, vmax = 1, annot = True, cmap = 'crest')\n\nplt.title('Correlation Heatmap Interested Variables', fontsize = '18')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Logistic Regression models are very sensitive to outliers, so we will remove the ones from tenure that were identified in previous steps.","metadata":{}},{"cell_type":"code","source":"df_logreg = df_enc[(df_enc['tenure'] > lower_limit) & (df_enc['tenure'] < upper_limit)]\n\ndf_logreg.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Determine dependant variable:","metadata":{}},{"cell_type":"code","source":"y = df_logreg['left']\ny.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Determine the dependants variables:","metadata":{}},{"cell_type":"code","source":"X = df_logreg.drop('left', axis = 1)\nX.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split our data in train and test groups:","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Construct the model and fit the train data:","metadata":{}},{"cell_type":"code","source":"log_clf = LogisticRegression(random_state = 42, max_iter = 500)\n\nlog_clf.fit(X_train, y_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we use the model to make predictions on the test data:","metadata":{}},{"cell_type":"code","source":"y_pred = log_clf.predict(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will display the predicted results vs the y_test on a Confusion Matrix, to visualize the distribution of the results of the model:","metadata":{}},{"cell_type":"code","source":"log_cm = confusion_matrix(y_test, y_pred, labels = log_clf.classes_)\n\nlog_disp = ConfusionMatrixDisplay(confusion_matrix = log_cm, display_labels = log_clf.classes_)\n\nlog_disp.plot()\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quadrants display:\n1. Upper left: number of true negatives.\n2. Bottom right: number of true positives.\n3. Upper right: number of false positives.\n4. Bottom left: number of false negatives.\n\n\nMeaning if each quadrant:\n1. True negatives: number of employees the model predicted that would stay, and actually stayed.\n2. True positives: number of employees the model predicted that would leave, and actually left.\n3. False positives: number of emplyees the model predicted that would leave, and actually stayed.\n4. False negatives: number of employees the model predicted that would stay, and actually left.\n\nA perfect model would yield true positives and true negatives, and no false positives and false negatives.\n\n","metadata":{}},{"cell_type":"markdown","source":"Now we can evaluate the model performance by accuracy, precision, recall and f1.","metadata":{}},{"cell_type":"markdown","source":"First we will check the balance of the data. Since this is a binary classification task, the class balance informs the way you interpret the accuracy metrics.","metadata":{}},{"cell_type":"code","source":"df_logreg['left'].value_counts(normalize = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data on the dependant variable is inbalance, ~83% of the observation stayed and ~17% left.\nThe data is not perfectly balanced (50-50), but is not extremely inbalanced that we would need to resample or modifying the data. We can continue to evaluate the model.","metadata":{}},{"cell_type":"code","source":"target_names = ['Predicted would not leave', 'Predicted would leave']\nprint(classification_report(y_test, y_pred, target_names = target_names))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tree-based Model","metadata":{}},{"cell_type":"markdown","source":"In this approach we will implement Decision Tree and Random Forest.","metadata":{}},{"cell_type":"markdown","source":"Import necessary libraries, packages and modules.","metadata":{}},{"cell_type":"code","source":"# For data modeling.\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# For metrics and helpful functions.\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import plot_tree\n\n# For saving models\nimport pickle","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will encode the categorical variables:","metadata":{}},{"cell_type":"code","source":"df2 = pd.get_dummies(df1)\ndf2.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the outcome variable:","metadata":{}},{"cell_type":"code","source":"y = df2['left']\ny.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define features:","metadata":{}},{"cell_type":"code","source":"X = df2.drop('left', axis = 1)\nX.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split data into train, validate and test sets:","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 0)\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.25, stratify = y_train, random_state = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decision Tree - Round 1","metadata":{}},{"cell_type":"markdown","source":"Construct a decision tree model and set up cross-validation grid-search, to find best parameters.","metadata":{}},{"cell_type":"code","source":"# Instantiate the model\ntree = DecisionTreeClassifier(random_state = 0)\n\n# Assign a dictionary of hyperparameters to search over\ncv_params = {'max_depth': [4, 6, 8, None ],\n            'min_samples_leaf': [2, 5, 1],\n            'min_samples_split': [2, 4, 6],\n            }\n# Assign a dictionary of scoring metrics to evaluate\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# Instantiate GridSearch\ntree1 = GridSearchCV(tree, cv_params, scoring = scoring, cv = 4, refit = 'roc_auc')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fit the model to the training data.","metadata":{}},{"cell_type":"code","source":"%%time\ntree1.fit(X_tr, y_tr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Identify the best values for the decision tree hyperparameters","metadata":{}},{"cell_type":"code","source":"tree1.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Obtain the best AUC score achieved by the decision tree model on the training set","metadata":{}},{"cell_type":"code","source":"tree1.best_score_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The AUC score is really good, so the model is really good at predicting which employees will leave.","metadata":{}},{"cell_type":"markdown","source":"We will oberve all the scores that the model achieve.","metadata":{}},{"cell_type":"code","source":"def make_results(model_name: str, model_object, metric: str):\n    \"\"\"\n    Arguments:\n        model_name(string): what you want the model to be called in the output table.\n        model_object: a fit GridSearchCV object.\n        metric(string): precision, recall, f1, accuracy or auc.\n        \n    Returns a pandas dataframe with the precision, recall, accuracy, f1, and auc scores\n    for the model with the best mean metric score across all validation folds.\n    \"\"\"\n    \n    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n    metric_dict = {'auc': 'mean_test_roc_auc',\n                  'precision': 'mean_test_precision',\n                  'recall': 'mean_test_recall',\n                  'f1': 'mean_test_f1',\n                  'accuracy': 'mean_test_accuracy'\n                  }\n    \n    # Get all the results from the CV and put them in a datafram\n    cv_results = pd.DataFrame(model_object.cv_results_)\n    \n    # Isolate the row of the dataframe with the max(metric) score\n    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n    \n    # Extract accuracy, precision, recall, f1 and auc from that row\n    accuracy = best_estimator_results.mean_test_accuracy\n    precision = best_estimator_results.mean_test_precision\n    recall = best_estimator_results.mean_test_recall\n    f1 = best_estimator_results.mean_test_f1\n    auc = best_estimator_results.mean_test_roc_auc\n    \n    # Create table of results\n    table = pd.DataFrame()\n    table = table.append({'Model': model_name,\n                         'AUC': auc,\n                         'Precision': precision,\n                         'Recall': recall,\n                         'F1': f1,\n                         'Accuracy': accuracy\n                         },\n                            ignore_index = True\n                        )\n    return table\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets obtain the scores from the Grid Search","metadata":{}},{"cell_type":"code","source":"tree1_cv_results = make_results('Decision Tree CV', tree1, 'auc')\ntree1_cv_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the performance metric are high, this indicates that the model is strong.\n\nWe need to remember that Decision-Trees are vulnerable to overfitting. \n\nRandom Forests avoid overfitting by incorporating multiple trees to make predictions. We should evaluate that model.","metadata":{}},{"cell_type":"markdown","source":"#### Random Forest - Round 1","metadata":{}},{"cell_type":"markdown","source":"Lets construct a Random Forest model and set up Cross-Validation GridSearch to search for the best model parameters.","metadata":{}},{"cell_type":"code","source":"# Instantiate model\nrf = RandomForestClassifier(random_state = 0)\n\n# Assign a dictionary of hyperparameters to search over\ncv_params = {'max_depth': [3, 5, None],\n             'max_features': [1.0],\n            'max_samples': [0.7, 1.0],\n            'min_samples_leaf': [1, 2, 3],\n            'min_samples_split': [2, 3, 4],\n            'n_estimators': [300, 500]\n            }\n\n# Assign a dictionary of scoring metrics to capture\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# Instantiate GridSearch\nrf1 = GridSearchCV(rf, cv_params, scoring = scoring, cv = 4, refit = 'roc_auc')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We fit the model to the training data.","metadata":{}},{"cell_type":"code","source":"%%time\nrf1.fit(X_tr, y_tr) # Wall time: ~9 min.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We specify a path to save our model.","metadata":{}},{"cell_type":"code","source":"#path = '/your/file/path/'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def write_pickle(path, model_object, save_as: str):\n#    \"\"\"\n#    In:\n#        path: path of the folder where you want to save the pickle\n#        model_object: a model you want to pickle\n#        save_as: filename for how you want to save the model\n        \n#    Out: a call to pickle the model in the folder indicated \n#    \"\"\"\n    \n#    with open(path + save_as + '.pickle', 'wb') as to_write:\n#        pickle.dump(model_object, to_write)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#def read_pickle(path, saved_model_name: str):\n#    \"\"\"\n#    In:\n#        path:path to the folder where you want to read from\n#        saved_model_name: filename of pickle model you want to read\n        \n#    Out:\n#        model: the pickled model\n#    \"\"\"\n    \n#    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n#        model = pickle.load(to_read)\n        \n#    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use the functions defined previously to save the model in a pickle fil and then read it in.","metadata":{}},{"cell_type":"code","source":"# write_pickle(path, rf1, 'hr_rf1')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rf1 = read_pickle(path, 'hr_rf1')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets obtain the best AUC score achieved by the Random Forest model on the training set.","metadata":{}},{"cell_type":"code","source":"rf1.best_score_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets obtain the optimal hyperparameters values","metadata":{}},{"cell_type":"code","source":"rf1.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we collect the evaluation scores on the training data sets from Decision Tree and Random Forest","metadata":{}},{"cell_type":"code","source":"rf1_cv_results = make_results('Random Forest CV', rf1, 'auc')\nprint(tree1_cv_results)\nprint(rf1_cv_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All Random Forest scores are better than the Decision Tree model, except for Recall, which is almost the same. This indicates that the Random Forest Model outperformed the Decision Tree Model.","metadata":{}},{"cell_type":"markdown","source":"We will continue to evaluate the models on the validation set.","metadata":{}},{"cell_type":"markdown","source":"Lets define a function that obtains the all the scores of a models predictions.","metadata":{}},{"cell_type":"code","source":"def get_scores(model_name: str, model, X_test_data, y_test_data):\n    \"\"\"\n    Generate a table of test scores.\n    \n    In: \n        model_name (string): how you want the model to be named in the output table\n        model: a fitGridSearchCV object\n        X_test_data: numpy array of X_test data\n        y_test_data: numpy array of y_test data\n    \n    Out: a pandas dataframe with accuracy, precision, recall, f1, and AUC scores for your model\n    \"\"\"\n    \n    preds = model.best_estimator_.predict(X_test_data)\n    \n    auc = round(roc_auc_score(y_test_data, preds), 3)\n    accuracy = round(accuracy_score(y_test_data, preds), 3)\n    precision = round(precision_score(y_test_data, preds), 3)\n    recall = round(recall_score(y_test_data, preds), 3)\n    f1 = round(f1_score(y_test_data, preds), 3)\n    \n    table = pd.DataFrame({'model': [model_name],\n                         'AUC': [auc],\n                         'precision': [precision],\n                         'recall': [recall],\n                         'f1': [f1],\n                         'accuracy': [accuracy]\n                         }\n                        )\n    \n    return table","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets get the the results using the table recently define","metadata":{}},{"cell_type":"code","source":"# Get the results on the validation set for both models\ntree1_val_results = get_scores('decision tree1 val', tree1, X_val, y_val)\nrf1_val_results = get_scores('random forest1 val', rf1, X_val, y_val)\n\n# Concatenate validation scores into table\nall_val_results1 = [tree1_val_results, rf1_val_results]\nall_val_results1 = pd.concat(all_val_results1).sort_values(by = 'AUC', ascending = False)\nall_val_results1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The Random Forest has better metrics than the Decision Tree model on the validation data, except for a small difference on recall score.","metadata":{}},{"cell_type":"markdown","source":"We will use the the Random Forest model, that outperformend the Decision Tree model, to predict on the test set.","metadata":{}},{"cell_type":"code","source":"rf1_test_scores = get_scores('random forest 1 test', rf1, X_test, y_test)\nrf1_test_scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost all test scores are even better on the test sets. This model seems to be strong. We only used the test set for this model, so we can be confident that the performance data is representative of how it will perform in new data.","metadata":{}},{"cell_type":"markdown","source":"#### Feature Engeneering","metadata":{}},{"cell_type":"markdown","source":"The evaluations scores are really high, that makes as a little bit sckeptical. There could be some data leakage.\n\nIt's likely that the company won't have satisfaction levels reported for all of its employees. Also, is possible that the average_monthly_hours column is a source of some data leakage. If employees have already decided upon quitting, or have already identified by management as people to be fired, they may be working fewer hours. \n\nThe first round of decision tree and random forest models included all variables as features. Now we will incorporate feature engineering to build improved models.","metadata":{}},{"cell_type":"markdown","source":"We will drop satisfaction_level and create a new feature that roughly captures whether an employee is overworked. We will call feature overworked, it is a binary variable.","metadata":{}},{"cell_type":"code","source":"df3 = df1.drop('satisfaction_level', axis = 1)\ndf3.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df3['overworked'] = df3['average_monthly_hours']\n\nprint('Min Avg. Hours:', df3['overworked'].min())\nprint('Max Avg. Hours:', df3['overworked'].max())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"International standards indicates that the average monthly hours should be 168.\n\nWe will define being overworked when hours are over 175.","metadata":{}},{"cell_type":"code","source":"# Transoform overworked to a boolean variable (True, False)\ndf3['overworked'] = df3['overworked'] > 175\n\n#Â Transform True and False to 1 and 0\ndf3['overworked'] = df3['overworked'].astype(int)\ndf3.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can drop average_monthly_hours","metadata":{}},{"cell_type":"code","source":"df3 = df3.drop('average_monthly_hours', axis = 1)\ndf3.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets hot-encode the categorical variables","metadata":{}},{"cell_type":"code","source":"df4 = pd.get_dummies(df3, drop_first = False)\ndf4.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets define the outcome variable.","metadata":{}},{"cell_type":"code","source":"y = df4['left']\ny.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define the features.","metadata":{}},{"cell_type":"code","source":"X = df4.drop('left', axis = 1)\nX.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split data.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 0)\n\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size = 0.25, stratify = y_train, random_state = 0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Descision Tree - Round 2","metadata":{}},{"cell_type":"code","source":"# Instantiate the model\ntree = DecisionTreeClassifier(random_state = 0)\n\n# Define dictionary of hyperparameters\ncv_params = {'max_depth': [4, 6, 8, None],\n            'min_samples_leaf': [2, 5, 1],\n            'min_samples_split': [2, 4, 6]\n            }\n\n# Define scoring dictionary\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# Instantiate GridSearchCV\ntree2 = GridSearchCV(tree, cv_params, scoring = scoring, cv = 4, refit = 'roc_auc')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntree2.fit(X_tr, y_tr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree2.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tree2.best_score_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This model still performs very well, with the modifications we have made.","metadata":{}},{"cell_type":"markdown","source":"Let's check other scores.","metadata":{}},{"cell_type":"code","source":"tree2_cv_results = make_results('Decision Tree2 CV', tree2, 'auc')\ntree2_cv_results","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the scores are lower. Thats to be expected for the changes we made (fewer features). The model still performs very well.","metadata":{}},{"cell_type":"markdown","source":"#### Random Forest - Round 2","metadata":{}},{"cell_type":"code","source":"# Instantiate the model\nrf = RandomForestClassifier(random_state = 0)\n\n# Define hyperparameters dictionary\ncv_params = {'max_depth': [3, 5, None],\n            'max_features': [1.0],\n            'max_samples': [0.7, 1.0],\n            'min_samples_leaf': [1, 2, 3],\n            'min_samples_split': [2, 3, 4],\n            'n_estimators': [300, 500]\n            }\n\n# Define scoring dictionary\nscoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n\n# Instantiate GridSearch\nrf2 = GridSearchCV(rf, cv_params, scoring = scoring, cv = 4, refit = 'roc_auc')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nrf2.fit(X_tr, y_tr) # Wall time: ~6 min.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write_pickle(path, rf2, 'hr_rf2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rf2 = read_pickle(path, 'hr_rf2')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf2.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf2.best_score_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see the other scores.","metadata":{}},{"cell_type":"code","source":"rf2_cv_results = make_results('Random Forest2 CV', rf2, 'auc')\nprint(tree2_cv_results)\nprint(rf2_cv_results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The scores on the Random Forest are a little bit lower, but still performs really good. Random Forest still outperformed Decision Tree model.","metadata":{}},{"cell_type":"markdown","source":"Now we test the models on the validation sets.","metadata":{}},{"cell_type":"code","source":"# Collect validation scores\ntree2_val_results = get_scores('Decision Tree2 Val', tree2, X_val, y_val)\nrf2_val_results = get_scores('Random Forest2 Val', rf2, X_val, y_val)\n\n# Concatenate validation scores into table\nall_val_results2 = [tree2_val_results, rf2_val_results]\nall_val_results2 = pd.concat(all_val_results2).sort_values(by = 'AUC', ascending = False)\nall_val_results2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems that Random Forest perform slightly better across most evaluation metrics.","metadata":{}},{"cell_type":"markdown","source":"Lets use the Random Forest to predict on the test set.","metadata":{}},{"cell_type":"code","source":"# Get predictions on test data\nrf2_test_scores = get_scores('Random Forest2 test', rf2, X_test, y_test)\nrf2_test_scores","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now lets use Random Forest model to predict on the test set.","metadata":{}},{"cell_type":"code","source":"preds = rf2.best_estimator_.predict(X_test)\ncm = confusion_matrix(y_test, preds, labels = rf2.classes_)\n\ndisp = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = rf2.classes_)\n\ndisp.plot()\n\nplt.show()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets continue to explore the splits on the Decision Tree and the most important features in the Random Forest.","metadata":{}},{"cell_type":"code","source":"#Â Plot the tree\nplt.figure(figsize = (85, 20))\nplot_tree(tree2.best_estimator_, max_depth = 6, fontsize = 14, feature_names = X.columns,\n         class_names = {0: 'stayed', 1: 'left'}, filled = True)\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Decision Tree feature importance","metadata":{}},{"cell_type":"markdown","source":"We can also get the feature importance on a Decision Tree model.","metadata":{}},{"cell_type":"code","source":"# tree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, columns = X.columns)\ntree2_importances = pd.DataFrame(tree2.best_estimator_.feature_importances_, columns = ['gini_importance'], \n                                 index = X.columns)\ntree2_importances = tree2_importances.sort_values(by = 'gini_importance', ascending = False)\n\n# Only extract the features with importances > 0\ntree2_importances = tree2_importances[tree2_importances['gini_importance'] > 0]\ntree2_importances","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets visualize them in a bar plot.","metadata":{}},{"cell_type":"code","source":"sns.barplot(data = tree2_importances,\n           x = 'gini_importance',\n           y = tree2_importances.index)\n\nplt.title('Decision Tree: Feature Importances', fontsize = '14')\nplt.ylabel('Features')\nplt.xlabel('Importance')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The most important features, in this model, for predicting which employees will leave are: \n1. Number of projects.\n2. Last evaluation.\n3. Tenure.\n4. Overworked","metadata":{}},{"cell_type":"markdown","source":"#### Random Forest featur importances","metadata":{}},{"cell_type":"code","source":"# Get feature importances\nfeat_impt = rf2.best_estimator_.feature_importances_\n\n# Get indices of the top 10 features\nind = np.argpartition(rf2.best_estimator_.feature_importances_, -10)[-10:]\n\n# Get column labels of the top 10 features\nfeat = X.columns[ind]\n\n# Filter 'feat_impt' to consist of the top 10 feature importances\nfeat_impt = feat_impt[ind]\n\ny_df = pd.DataFrame({'Feature': feat, 'Importance': feat_impt})\ny_sort_df = y_df.sort_values('Importance')\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\n\ny_sort_df.plot(kind = 'barh', ax = ax1, x = 'Feature', y = 'Importance')\n\nax1.set_title('Random Forest: Feature Importances', fontsize = '14')\nax1.set_ylabel('Feature')\nax1.set_xlabel('Importance')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The previous visualization shows us the most important features, to this model, for prediciting which empoyees could leave the company are:\n1. Last evaluation.\n2. Number of projects.\n3. Tenure.\n4. Overworked.","metadata":{}},{"cell_type":"markdown","source":"![Screenshot 2022-08-04 2.05.04 PM.png](data:image/png;base64,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)\n# pacE: Execute Stage\n- Interpret model performance and results\n- Share actionable steps with stakeholders\n\n","metadata":{"id":"401PgchTPr4E"}},{"cell_type":"markdown","source":"## Evaluation metrics definitions\n\n- **AUC** is the area under the ROC curve; it's also considered the probability that the model ranks a random positive example more highly than a random negative example.\n- **Precision** measures the proportion of data points predicted as True that are actually True, in other words, the proportion of positive predictions that are true positives.\n- **Recall** measures the proportion of data points that are predicted as True, out of all the data points that are actually True. In other words, it measures the proportion of positives that are correctly classified.\n- **Accuracy** measures the proportion of data points that are correctly classified.\n- **F1-score** is an aggregation of precision and recall.\n\n\n\n\n","metadata":{"id":"ex8pgn5iNzau"}},{"cell_type":"markdown","source":"## Step 4. Results and Evaluation\n- Interpret model\n- Evaluate model performance using metrics\n- Prepare results, visualizations, and actionable steps to share with stakeholders\n\n\n","metadata":{"id":"6pBUk35yTDaL"}},{"cell_type":"markdown","source":"### Summary of model results\n\n#### Logistic Regression Model\n\nThe logistic regression model achieved the following wighted average scores on the test set:\n- Precision: 79%\n- Recall: 82%\n- F1-score: 80% \n- Accuracy: 82%\n\n#### Tree-based Machine Learning Model\n\nAfter conducting feature engineering, the decision tree model achieved the following results on the test set: \n- AUC: 94.2% \n- Precision: 88.3%\n- Recall: 90.7%\n- F1-score: 89.5%\n- Accuracy: 96.5%\n\n#### Random Forest Machine Learning Model\n\n- AUC: 93.3% \n- Precision: 90.5%\n- Recall: 88.4%\n- F1-score: 89.5%\n- Accuracy: 96.5%\n\n##### The random forest model slightly outperformed the decision tree model.","metadata":{"id":"GXrsxT498Z7h"}},{"cell_type":"markdown","source":"On the test set the Random Forest model performed as follows:\n\n- AUC: 93.5% \n- Precision: 89.8%\n- Recall: 88.9%\n- F1-score: 89.4%\n- Accuracy: 96.5%","metadata":{}},{"cell_type":"markdown","source":"### Conclusion, Recommendations, Next Steps\n\n- Control and limit the number of projects that an employee can work on simultaneously.\n- Conduct further investigations about why 4 year tenure employees are so dissatisfied.\n- Reward employees that work long hours and give them some kind of recognition (promotion, salary, bonuses, company awards).\n- Create  culture, incentives and polocies of healthier work/life balance.\n- High evaluation scores shouldnÂ´t be reserved only for eployees who work long hours.\n\n#### Next Steps\n- Address and conduct further verifications for data leakage.\n- Consider how predictions change when last_evaluation is removed from the data. ItÂ´s possible that evaluations arenÂ´t performed frequently.It colud be useful to predict retention whithout this feature.\n- There could further analysis building K-means model and analyzing the clusters, this may give valuable insights.","metadata":{"id":"9MOMqelNLn2v"}}]}